<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#intro">Intro</a></li>
<li><a href="#what_is_new">What is new</a></li>
<li><a href="#getting_started">Getting started</a><ul>
<li><a href="#Maven">Maven</a></li>
<li><a href="#Hello_World">Hello World</a></li>
<li><a href="#What_you_should_know">What you should know</a></li>
</ul></li>
<li><a href="#dbmaker_and_db">`DBMaker and DB</a><ul>
<li><a href="#Transactions">Transactions</a></li>
</ul></li>
<li><a href="#quick_intro_to_mapdb_internals">Quick intro to MapDB internals</a><ul>
<li><a href="#Dictionary">Dictionary</a></li>
<li><a href="#DBMaker_and_DB">DBMaker and DB</a></li>
<li><a href="#Layers">Layers</a></li>
<li><a href="#Volume">Volume</a></li>
<li><a href="#Store">Store</a></li>
<li><a href="#Engine_Wrappers">Engine Wrappers</a></li>
<li><a href="#TxMaker">TxMaker</a></li>
<li><a href="#Collections">Collections</a></li>
<li><a href="#Serialization">Serialization</a></li>
<li><a href="#Concurrency_patterns">Concurrency patterns</a></li>
</ul></li>
<li><a href="#durability_and_speed">Durability and speed</a><ul>
<li><a href="#Transactions_disabled">Transactions disabled</a></li>
<li><a href="#Memory_mapped_files_mmap">Memory mapped files (mmap)</a></li>
<li><a href="#CRC32_checksums">CRC32 checksums</a></li>
</ul></li>
<li><a href="#caches">Caches</a><ul>
<li><a href="#Hash_Table_cache">Hash Table cache</a></li>
<li><a href="#Least-Recently-Used_cache">Least-Recently-Used cache</a></li>
<li><a href="#Hard_reference_cache">Hard reference cache</a></li>
<li><a href="#Soft_and_Weak_reference_cache">Soft and Weak reference cache</a></li>
<li><a href="#Disabled_cache_or_reduce_size">Disabled cache or reduce size</a></li>
<li><a href="#Clear_cache">Clear cache</a></li>
<li><a href="#Cache_hit_and_miss_statistics">Cache hit and miss statistics</a></li>
<li><a href="#Cache_priority">Cache priority</a></li>
</ul></li>
<li><a href="#btreemap">BTreeMap</a></li>
<li><a href="#htreemap">HTreeMap</a><ul>
<li><a href="#Parameters">Parameters</a></li>
<li><a href="#Entry_expiration_parameters">Entry expiration parameters</a></li>
<li><a href="#Compared_to_BTreeMap">Compared to BTreeMap</a></li>
</ul></li>
<li><a href="#queues">Queues</a><ul>
<li><a href="#FIFO_Queue">FIFO Queue</a></li>
<li><a href="#Stack">Stack</a></li>
<li><a href="#CircularQueue">CircularQueue</a></li>
</ul></li>
<li><a href="#secondary_collections">Secondary Collections</a><ul>
<li><a href="#Consistency">Consistency</a></li>
<li><a href="#Performance">Performance</a></li>
</ul></li>
</ul>
</div>
<hr />
<ul>
<li><a href="#intro" title="Intro">Intro</a></li>
</ul>
<h1 id="intro"><a href="#intro">Intro</a></h1>
<p>TODO intro text based on intro video</p>
<p>TODO link intro video</p>
<hr />
<ul>
<li><a href="#what_is_new" title="What is new">What is new</a></li>
</ul>
<h1 id="what_is_new"><a href="#what_is_new">What is new</a></h1>
<p>TODO this will be change-log after 1.0.0 is released.</p>
<hr />
<ul>
<li><a href="#getting_started" title="Getting started">Getting started</a></li>
<li><a href="#Maven" title="Maven">Maven</a></li>
<li><a href="#Hello_World" title="Hello World">Hello World</a></li>
<li><a href="#What_you_should_know" title="What you should know">What you should know</a></li>
</ul>
<h1 id="getting_started"><a href="#getting_started">Getting started</a></h1>
<p>MapDB has very power-full API, but for 99% cases you need just two classes: <a href="http://www.mapdb.org/apidocs/org/mapdb/DBMaker.html">DBMaker</a> is builder style factory for configuring and opening a database. It has handful of static ‘newXXX’ methods for particular storage mode. <a href="http://www.mapdb.org/apidocs/org/mapdb/DB.html">DB</a> represents storage. It has methods for accessing Maps and other collections. It also controls DB life-cycle with commit, rollback and close methods.</p>
<p>Best place to checkout various features of MapDB are <a href="https://github.com/jankotek/MapDB/tree/master/src/test/java/examples">Examples</a>. There is also <a href="http://www.youtube.com/watch?v=FdZmyEHcWLI">screencast</a> which describes most aspects of MapDB.</p>
<p>There is <a href="http://www.mapdb.org/doc/cheatsheet.pdf">MapDB Cheat Sheet</a>, on just two pages it is quick reminder of MapDB capabilities.</p>
<h2 id="Maven"><a href="#Maven">Maven</a></h2>
<p>MapDB is in Maven Central. Just add code bellow to your pom file to use it. You may also download jar file directly from <a href="http://search.maven.org/#browse%7C845836981">repo</a>.</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.mapdb&lt;/groupId&gt;
    &lt;artifactId&gt;mapdb&lt;/artifactId&gt;
    &lt;version&gt;1.0.5&lt;/version&gt;
&lt;/dependency&gt;</code></pre>
<p>There is also repository with <a href="https://oss.sonatype.org/content/repositories/snapshots/org/mapdb/mapdb/">daily builds</a>:</p>
<pre><code>&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;sonatype-snapshots&lt;/id&gt;
        &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.mapdb&lt;/groupId&gt;
        &lt;artifactId&gt;mapdb&lt;/artifactId&gt;
        &lt;version&gt;1.1.0-SNAPSHOT&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
<h2 id="Hello_World"><a href="#Hello_World">Hello World</a></h2>
<p>Hereafter is a simple example. It opens TreeMap backed by file in temp directory, file is discarded after JVM exit:</p>
<pre><code>import org.mapdb.*;
ConcurrentNavigableMap treeMap = DBMaker.newTempTreeMap()

// and now use disk based Map as any other Map
treeMap.put(111,&quot;some value&quot;)</code></pre>
<p>More advanced example with configuration and write-ahead-log transaction.</p>
<pre><code>    import org.mapdb.*;

    // configure and open database using builder pattern.
    // all options are available with code auto-completion.
    DB db = DBMaker.newFileDB(new File(&quot;testdb&quot;))
               .closeOnJvmShutdown()
               .encryptionEnable(&quot;password&quot;)
               .make();

    // open existing an collection (or create new)
    ConcurrentNavigableMap&lt;Integer,String&gt; map = db.getTreeMap(&quot;collectionName&quot;);

    map.put(1, &quot;one&quot;);
    map.put(2, &quot;two&quot;);
    // map.keySet() is now [1,2]

    db.commit();  //persist changes into disk

    map.put(3, &quot;three&quot;);
    // map.keySet() is now [1,2,3]
    db.rollback(); //revert recent changes
    // map.keySet() is now [1,2]

    db.close();</code></pre>
<h2 id="What_you_should_know"><a href="#What_you_should_know">What you should know</a></h2>
<p>MapDB is very simple to use, however it bites when used wrong way. Here is list of most common usage errors and things to avoid:</p>
<ul>
<li><p>Transactions (write-ahead-log) can be disabled with <a href="http://www.mapdb.org/apidocs/org/mapdb/DBMaker.html#transactionDisable()">DBMaker.transactionDisable()</a>, this will speedup writes. However without transactions store gets corrupted when not closed correctly.</p></li>
<li><p>Keys and values must be immutable. MapDB may serialize them on background thread, put them into instance cache… Modifying an object after it was stored is a bad idea.</p></li>
<li><p>MapDB relies on memory mapped files. On 32bit JVM you will need <a href="http://www.mapdb.org/apidocs/org/mapdb/DBMaker.html#randomAccessFileEnable()">DBMaker.randomAccessFileEnable()</a> configuration option to access files larger than 2GB. RAF introduces overhead compared to memory mapped files.</p></li>
<li><p>MapDB does not run defrag on background. You need to call <code>DB.compact()</code> from time to time.</p></li>
<li><p>MapDB uses unchecked exceptions. All <code>IOException</code> are wrapped into unchecked <code>IOError</code>. MapDB has weak error handling and assumes disk failure can not be recovered at runtime. However this does not affects data safety, if you use durable commits.</p></li>
</ul>
<hr />
<ul>
<li><a href="#dbmaker_and_db" title="`DBMaker and DB">`DBMaker and DB</a></li>
<li><a href="#Transactions" title="Transactions">Transactions</a></li>
</ul>
<h1 id="dbmaker_and_db"><a href="#dbmaker_and_db">`DBMaker and DB</a></h1>
<p>MapDB is set of loosely coupled components. One could wire classes such as <code>CacheMRU</code>, <code>StoreWAL</code> and <code>BTreeMap</code> manually, byt there are two factory classes to do it for you: <code>DBMaker</code> and <code>DB</code>. They use maker (builder) pattern, so most configuration options are quickly available via code assistant in IDE.</p>
<p><a href="apidocs/org/mapdb/DBMaker.html">DBMaker</a> handles database configuration, creation and opening. MapDB has several modes and configuration options. Most of those can be set using this class.</p>
<p><a href="apidocs/org/mapdb/DB.html">DB</a> represents opened database (or single transaction session). It creates and opens collections . It also handles transaction with methods such as <code>commit()</code>, <code>rollback()</code> and <code>close()</code>.</p>
<p>To open (or create) store use one of <code>DBMaker.newXXX()</code> static methods. MapDB has more formats and modes, each <code>newXXX()</code> uses different: <code>newMemoryDB()</code> opens in-memory database backed by <code>byte[]</code>, <code>newAppendFileDB()</code> opens db which uses append-only log files and so on.</p>
<p><code>newXXX()</code> method is followed by configuration options and <code>make()</code> method which applyes all options and returns <code>DB</code> object. This example opens file storage with encryption enabled:</p>
<pre><code>  DB db = DBMaker
    .newAppendFileDB(new File(&quot;/some/file&quot;))
    .encryptionEnable(&quot;password&quot;)
    .make();</code></pre>
<p>Once you have DB you may open collection or other record. DB has two types of factory methods:</p>
<p><code>getXXX()</code> opens existing collection (or record). If collection with given name does not exist, it is silently created with default settings and returned. An example:</p>
<pre><code>  NavigableSet treeSet = db.getTreeSet(&quot;treeSet&quot;);</code></pre>
<p><code>createXXX()</code> creates new collection (or settings) with customized settings. Specialized serializers, node size, entry compression and so on affect performance a lot and they are customizable here.</p>
<pre><code>  Atomic.Var&lt;Person&gt; var = = db.createAtomicVar(&quot;mainPerson&quot;, Person.SERIALIZER);</code></pre>
<p>Some <code>create</code> method may use builder style configuration. In that case you may finish with two methods: <code>make()</code> creates new collection, if collection with given name already exists it throws an exception. <code>makerOrGet()</code> is same, except if collection already exist it does not fail, but returns existing collection.</p>
<pre><code>  NavigableSet&lt;String&gt; treeSet = db.createTreeSet(&quot;treeSet);
    .nodeSize(112)
    .serializer(BTreeKeySerializer.STRING) 
    .makeOrGet();</code></pre>
<h2 id="Transactions"><a href="#Transactions">Transactions</a></h2>
<p><code>DB</code> has methods to handle transaction lifecycle: <code>commit()</code>, <code>rollback()</code> and <code>close()</code>.</p>
<pre><code>  ConcurrentNavigableMap&lt;Integer,String&gt; map = db.getTreeMap(&quot;collectionName&quot;);

  map.put(1,&quot;one&quot;);
  map.put(2,&quot;two&quot;);
  //map.keySet() is now [1,2] even before commit

  db.commit();  //persist changes into disk

  map.put(3,&quot;three&quot;);
  //map.keySet() is now [1,2,3]
  db.rollback(); //revert recent changes
  //map.keySet() is now [1,2]

  db.close();</code></pre>
<p>One <code>DB</code> object represents single transactions. Examples above use single global transaction, which is sufficient for some usages. MapDB support concurrent transactions as well with full serializable isolation, optimistic locking and MVCC snapshots. In that case we need one extra factory which creates transactions: <code>TxMaker</code>. We use <code>DBMaker</code> to create it, but instead of <code>make()</code> we call <code>makeTxMaker()</code></p>
<pre><code>  TxMaker txMaker = DBMaker
    .newMemoryDB()
    .makeTxMaker();</code></pre>
<p>And <code>TxMaker</code> is than used to create multiple <code>DB</code> objects, each representing single transaction:</p>
<p>```java DB tx0 = txMaker.makeTx(); Map map0 = tx0.getTreeMap(“testMap”); map0.put(0,“zero”);</p>
<p>DB tx1 = txMaker.makeTx(); Map map1 = tx1.getTreeMap(“testMap”);</p>
<p>DB tx2 = txMaker.makeTx(); Map map2 = tx1.getTreeMap(“testMap”);</p>
<p>map1.put(1,“one”); map2.put(2,“two”);</p>
<p>//each map sees only its modifications, //map1.keySet() contains [0,1] //map2.keySet() contains [0,2]</p>
<p>//persist changes tx1.commit(); tx2.commit();<br /> // second commit fails with write conflict, both maps share single BTree node, // this does not happend on large maps with sufficent number of BTree nodes.</p>
<hr />
<ul>
<li><a href="#quick_intro_to_mapdb_internals" title="Quick intro to MapDB internals">Quick intro to MapDB internals</a></li>
<li><a href="#Dictionary" title="Dictionary">Dictionary</a></li>
<li><a href="#DBMaker_and_DB" title="DBMaker and DB">DBMaker and DB</a></li>
<li><a href="#Layers" title="Layers">Layers</a></li>
<li><a href="#Volume" title="Volume">Volume</a></li>
<li><a href="#Store" title="Store">Store</a></li>
<li><a href="#Engine_Wrappers" title="Engine Wrappers">Engine Wrappers</a></li>
<li><a href="#TxMaker" title="TxMaker">TxMaker</a></li>
<li><a href="#Collections" title="Collections">Collections</a></li>
<li><a href="#Serialization" title="Serialization">Serialization</a></li>
<li><a href="#Concurrency_patterns" title="Concurrency patterns">Concurrency patterns</a></li>
</ul>
<h1 id="quick_intro_to_mapdb_internals"><a href="#quick_intro_to_mapdb_internals">Quick intro to MapDB internals</a></h1>
<p>This chapter gives 5 minutes introduction to MapDB internal architecture. Rest of this manual assume that you have read this chapter.</p>
<p>MapDB originally evolved as store for astronomical desktop application. As such it has a bit different design from most DBs. Major goal was to minimise overhead of any sort (garbage collection, memory, CPU, stack trace length…).</p>
<p>Also serialization lifecycle is very different here. In most DB engines user has to serialize data himself and pass binary data into db. API than looks similar to this:</p>
<pre><code>    engine.update(long recid, byte[] data);</code></pre>
<p>But MapDB serializes data itself by using user supplied serializer:</p>
<pre><code>    engine.update(long recid, Person data, Serializer&lt;Person&gt; serializer);</code></pre>
<p>So serialization lifecycle is driven by MapDB rather than by user. This small detail is reason why MapDB is so flexible. For example <code>update</code> method could pass data-serializer pair to <a href="apidocs/org/mapdb/AsyncWriteEngine.html">background-writer</a> thread and return almost instantly. Or <code>Person</code> instance could be stored in <a href="apidocs/org/mapdb/Caches.html">instance cache</a>, to minimise deserilization overhead on multiple reads. <code>Person</code> does not even have to be serialized, but could be stored in <code>Map&lt;Long,Person&gt;</code> map <a href="apidocs/org/mapdb/StoreHeap.html">on heap</a>, in this case MapDB has speed comparable to Java Collections.</p>
<p>Colons can be used to align columns.</p>
<h2 id="Dictionary"><a href="#Dictionary">Dictionary</a></h2>
<table>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="left">Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Record</td>
<td align="left">Atomically stored value. Usually tree node or similar. Transaction conflicts and locking is usually per record.</td>
</tr>
<tr class="even">
<td align="left">Index Table</td>
<td align="left">Table which translates recid into real offset and size in physical file.</td>
</tr>
<tr class="odd">
<td align="left"><a href="apidocs/org/mapdb/Engine.html">Engine</a></td>
<td align="left">Primitive key-value store used by collections for storage. Most features are <a href="apidocs/org/mapdb/EngineWrapper.html">Engine Wrappers</a></td>
</tr>
<tr class="even">
<td align="left"><a href="apidocs/org/mapdb/Store.html">Store</a></td>
<td align="left">Engine implementation which actually persist data. Is wrapped by other Engines.</td>
</tr>
<tr class="odd">
<td align="left"><a href="apidocs/org/mapdb/Volume.html">Volume</a></td>
<td align="left">Abstraction over ByteBuffer or other raw data store. Used for files, memory, partition etc..</td>
</tr>
<tr class="even">
<td align="left">Slice</td>
<td align="left">Non overlapping pages used in Volume. Slice size is 1MB. Older name was ‘chunk’</td>
</tr>
<tr class="odd">
<td align="left">Direct mode</td>
<td align="left">With disabled transactions, data are written directly into file. It is fast, but store is not protected from corruption during crasches.</td>
</tr>
<tr class="even">
<td align="left">WAL</td>
<td align="left">Write Ahead Log, way to protect store from corruption if db crashes during write.</td>
</tr>
<tr class="odd">
<td align="left">RAF</td>
<td align="left">Random Access File, way to access data on disk. Safer but slower method.</td>
</tr>
<tr class="even">
<td align="left">MMap file</td>
<td align="left">memory mapped file. On 32bit platform has size limit around 2GB. Faster than RAF.</td>
</tr>
<tr class="odd">
<td align="left">index file</td>
<td align="left">contains mapping between recid (index file offset) and record size and offset in physical file (index value). Is organized as sequence of 8-byte longs</td>
</tr>
<tr class="even">
<td align="left">index value</td>
<td align="left">single 8-byte number from index file. Usually contains record offset in physical file.</td>
</tr>
<tr class="odd">
<td align="left">recid</td>
<td align="left">record identificator, an unique 8-byte long number which identifies record. Recid is offset in index file. After record is deleted, its recid may be reused for newly inserted record.</td>
</tr>
<tr class="even">
<td align="left">record</td>
<td align="left">atomical value stored in storage (Engine) identified by record identifier (recid). In collections Record corresponds to tree nodes. In Maps record mey not correspond to Key-&gt;Value pair, as multiple keys may be stored inside single node.</td>
</tr>
<tr class="odd">
<td align="left">physical file</td>
<td align="left">Contains record binary data</td>
</tr>
<tr class="even">
<td align="left">cache (or instance cache)</td>
<td align="left">caches object instances (created with ‘new’ keyword). MapDB does not have traditional fixes-size-buffer cache for binary pages (it relies on OS to do this). Instead deserialized objects are cached on heap to minimise deserialization overhead. Instance cache is main reason why your keys/values must be immutable.</td>
</tr>
<tr class="odd">
<td align="left">BTreeMap</td>
<td align="left">tree implementation behind TreeMap and TreeSet provided by MapDB</td>
</tr>
<tr class="even">
<td align="left">HTreeMap</td>
<td align="left">tree implementation behind HashMap and HashSet provided by MapDB</td>
</tr>
<tr class="odd">
<td align="left">delta packing</td>
<td align="left">compression method to minimalise space used by keys in BTreeMap. Keys are sorted, so only difference between keys needs to be stored. You need to privide specialized serializer to enable delta packing.</td>
</tr>
<tr class="even">
<td align="left">append file db</td>
<td align="left">alternative storage format. In this case no existing data are modified, but all changes are appended to end of file. This may improve write speed and durability, but introduces some tradeoffs.</td>
</tr>
<tr class="odd">
<td align="left">temp map/set…</td>
<td align="left">collection backed by file in temporary directory. Is usually configured to delete file after close or on JVM exit. Data written into temp collection are not persisted between JVM restarts.</td>
</tr>
<tr class="even">
<td align="left">async write</td>
<td align="left">writes may be queued and written into file on background thread. This does not affect commit durability (it blocks until queue is empty).</td>
</tr>
<tr class="odd">
<td align="left">TX</td>
<td align="left">equals to Concurrent Transaction.</td>
</tr>
<tr class="even">
<td align="left">LongMap</td>
<td align="left">specialized map which uses primitive long for keys. It minimises boxing overhead.</td>
</tr>
<tr class="odd">
<td align="left">DB</td>
<td align="left">API class exposed by MapDB. It is an abstraction over Engine which manages MapDB collections and storage.</td>
</tr>
<tr class="even">
<td align="left">DMaker</td>
<td align="left">Builder style factory class, which opens and configures DB instances.</td>
</tr>
<tr class="odd">
<td align="left">Collection Binding</td>
<td align="left">MapDB mechanism to keep two collections synchronized. It provides secondary keys and values, aggregations etc.. known from SQL and other databases. All functions are provided as static methods in Bind class.</td>
</tr>
<tr class="even">
<td align="left">Data Pump</td>
<td align="left">Tool to import and manipulate large collections and storages.</td>
</tr>
</tbody>
</table>
<h2 id="DBMaker_and_DB"><a href="#DBMaker_and_DB">DBMaker and DB</a></h2>
<p>90% of users will only need two classes from MapDB. <a href="apidocs/org/mapdb/DBMaker.html">DBMaker</a> is builder style configurator which opens database. <a href="apidocs/org/mapdb/DB.html">DB</a> represents store, it creates and opens collections, commits or rollbacks data.</p>
<p>MapDB collections use <code>Engine</code> (simple key-value store) to persist its data and state. Most of functionality comes from mixing <code>Engine</code> implementations and wrappers. For example off-heap store with asynchronous writes and instance cache could be instantiated by this pseudo-code:</p>
<pre><code>    Engine engine = new Caches.HashTable(         //instance cache
                        new AsyncWriteEngine(     //asynchronous writes
                         new StoreWAL(            //actual store with WAL transactions
                          new Volume.MemoryVol()  //raw buffer used for storage
                    )))</code></pre>
<p>Reality is even more complex since each wrapper takes extra parameters and there are more levels. So <code>DBMaker</code> is a factory which takes settings and wires all MapDB classes together.</p>
<p><code>DB</code> has similar role. It is too hard to load and instantiate collections manually (for example <code>HTreeMap</code> constructor takes 14 parameters). So <code>DB</code> stores all settings in Named Catalog and handles collections. Named Catalog is <code>Map&lt;String,Object&gt;</code> which is persisted in store at fixed recid and contains parameters for all other named collections and named records. To rename collection one just has to rename relevant keys in Named Catalog.</p>
<h2 id="Layers"><a href="#Layers">Layers</a></h2>
<p>MapDB stack is little bit different from most DBs. It integrates instance cache and serialization usually found in ORM frameworks. On other side MapDB eliminated fixed-size page and disk cache.</p>
<p>From raw-files to <code>Map</code> interface it has following layers:</p>
<p>1) <strong>Volume</strong> - an <code>ByteBuffer</code> like abstraction over raw store. There are implementations for in-memory buffers or files.</p>
<p>2) <strong>Store</strong> - primitive key-value store (implementation of <code>Engine</code>). Key is offset on index table, value is variable length data. It has single transaction. Implementations are Direct, WAL, append-only and Heap (which does not use serialization). It performs serialization, encryption and compression.</p>
<p>3) <strong>AsyncWriterEngine</strong> - is optional <code>Store</code> (or <code>Engine</code>) wrapper which performs all modifications on background thread.</p>
<p>4) <strong>Instance Cache</strong> - is <code>Engine</code> wrapper which caches object instances. This minimises deserilization overhead.</p>
<p>5) <strong>TxMaker</strong> - is <code>Engine</code> factory which creates fake <code>Engine</code> for each transaction or snapshot. Dirty data are stored on heap.</p>
<p>6) <strong>Collections</strong> - such as TreeMap use <code>Engine</code> to store their data and state.</p>
<h2 id="Volume"><a href="#Volume">Volume</a></h2>
<p><code>ByteBuffer</code> is best raw buffer abstraction Java has. However its size is limited by 31 bits addressing to 2GB. For that purpose MapDB uses <code>Volume</code> as raw buffer abstraction. It takes multiple <code>ByteBuffer</code>s and uses them together with 64bit addressing. Each <code>ByteBuffer</code> has 1GB size and represents <em>slice</em>. IO operations which cross slice boundaries are not supported (<code>readLong(1GB-3)</code> will throw an exception). It is responsibility of higher layer <code>Store</code> to ensure data do not overlap slice boundaries.</p>
<p>MapDB provides some Volume implementations: heap buffers, direct (off-heap) buffers, memory mapped files and random access file. Each implementation fits different situation. For example memory mapped files have great performance, however 32bit desktop app will probably prefer random access files. All implementations share the same format, so it is possible to copy data (and entire store) between implementations.</p>
<p>User can also supply their own <code>Volume</code> implementations. For example each 1Gb slice can be stored in separate file on multiple disks, to create software RAID. <code>Volume</code> could also handle duplication, binary snapshots (MapDB snapshots are at different layer) or raw disks.</p>
<h2 id="Store"><a href="#Store">Store</a></h2>
<p><a href="apidocs/org/mapdb/Engine.html">Engine</a> (and <a href="apidocs/org/mapdb/Store.html">Store</a>) is primitive key-value store which maps recid (8-byte long record id) to some data (record). It has 4 methods for CRUD operations and 2 transaction methods:</p>
<pre><code>    long put(A, Serializer&lt;A&gt;)
    A get(long, Serializer&lt;A&gt;)
    void update(long, A, Serializer&lt;A&gt;)
    void delete(long, Serializer&lt;A&gt;)

    void commit()
    void rollback()</code></pre>
<p>By default MapDB stack supports only single transaction. However there is wrapper <code>TxMaker</code> which stores un-commited data on heap and provides concurrent ACID transactions.</p>
<p><a href="apidocs/org/mapdb/DB.html">DB</a> is low level implementation of <code>Engine</code> which stores data on raw <code>Volume</code>. It usually has two files (or Volumes): index table and physical file. Recid (record ID) is usually fixed offset in index table, which contains pointer to physical file.</p>
<p>MapDB has multiple <code>Store</code> implementations, which differ in speed and durability guarantees. User can also supply their own implementation.</p>
<p>First (and default) is <a href="apidocs/org/mapdb/StoreWAL.html">StoreWAL</a>. In this case Index Table contains record size and offset in physical file. Large records are stored as linked list. StoreWAL has free space management, so released space is reused. However over time it may require compaction. StoreWAL stores modifications in <em>Write Ahead Log</em>, which is sequence of simple instructions such as <em>write byte at this offset</em>. On commit (or reopen) WAL is replayed into main store, and discarded after successful file sync. On rollback the WAL is discarded.</p>
<p><a href="apidocs/org/mapdb/StoreDirect.html">StoreDirect</a> shares the same file format with <code>StoreWAL</code>, however it does not use write ahead log. Instead it writes data directly data into files and performs file sync on commit and close. This implementation trades any sort of data protection for speed, so data are usually lost if <code>StoreDirect</code> is not closed correctly (or synced after last write). Because there is no WAL, this store does not support rollback. This store is used if transactions are disabled.</p>
<p>Third implementation is <code>StoreAppend</code> which provides append-only file store. Because data are never overwritten, it is very solid and stable. However space usage skyrockets, since it stores all modifications ever made. TODO This store is not finished yet, so for example advanced compaction is missing. TODO Also all possibilities of this store are not explored (and documented yet). This store reads all data in sequence, in order to build Index Table which points to newest version of each record. The Index Table is stored on heap.</p>
<h2 id="Engine_Wrappers"><a href="#Engine_Wrappers">Engine Wrappers</a></h2>
<p>Big part of features in MapDB is implemented as <code>Engine</code> wrappers. For example <code>update</code> method does not have modify file directly, but it can forward modification into <a href="apidocs/org/mapdb/AsyncWriteEngine.html">background-writer</a></p>
<p>Also deserialized records can be stored in <a href="apidocs/org/mapdb/Caches.html">instance cache</a>, so it does not have to be deserialized on next read.</p>
<p>TODO expand Engine Wrappers section</p>
<h2 id="TxMaker"><a href="#TxMaker">TxMaker</a></h2>
<p>MapDB <code>Store</code>s support only single transaction. So concurrent transactions needs to be serialized and commited one by one. For this there is <a href="apidocs/org/mapdb/TxMaker.html">TxMaker</a>. It is factory which creates fake <code>Engine</code> for each transaction. Dirty (uncommited) data are stored on heap. Optimistic concurrency control is used to detect conflicts. <a href="apidocs/org/mapdb/TxRollbackException.html">TxRollbackException</a> is thrown on write or commit, if current transaction was rolled back thanks to an conflict.</p>
<p>TxMaker has Serializable Isolation level, this level supports highest guarantees. Other isolation levels are not implemented, since author does not want to support (and explain) isolation problems.</p>
<p>TODO Current TxMaker uses global lock, so concurrent performance sucks. It will be rewritten after 1.0 release.</p>
<h2 id="Collections"><a href="#Collections">Collections</a></h2>
<p>MapDB collection uses <code>Engine</code> as its parameter. There are two basic indexes:</p>
<p><a href="apidocs/org/mapdb/BTreeMap.html">BTreeMap</a> is ordered B-Linked-Tree. It offers great concurrent performance. It is best for small sized keys.</p>
<p><a href="apidocs/org/mapdb/HTreeMap.html">HTreeMap</a> is segmented Hash-Tree. It is good for large keys and values. It also supports entry expiration based on maximal size or time-to-live.</p>
<p>There also also <a href="apidocs/org/mapdb/Queues.html">Queues</a> and <a href="apidocs/org/mapdb/Atomic.html">Atomic</a> variables</p>
<p>TODO explain collections.</p>
<h2 id="Serialization"><a href="#Serialization">Serialization</a></h2>
<p>MapDB contains its own serialization framework. TODO explain serialization</p>
<h2 id="Concurrency_patterns"><a href="#Concurrency_patterns">Concurrency patterns</a></h2>
<p>TODO concurrency patterns.</p>
<hr />
<ul>
<li><a href="#durability_and_speed" title="Durability and speed">Durability and speed</a></li>
<li><a href="#Transactions_disabled" title="Transactions disabled">Transactions disabled</a></li>
<li><a href="#Memory_mapped_files_mmap" title="Memory mapped files (mmap)">Memory mapped files (mmap)</a></li>
<li><a href="#CRC32_checksums" title="CRC32 checksums">CRC32 checksums</a></li>
</ul>
<h1 id="durability_and_speed"><a href="#durability_and_speed">Durability and speed</a></h1>
<p>There are several configuration options to make compromises between durability and speed. You may choose consistency, disk access patterns, commit type, flush type and so on.</p>
<h2 id="Transactions_disabled"><a href="#Transactions_disabled">Transactions disabled</a></h2>
<p>If process dies in middle of write, storage files might become inconsistent. For example pointer was updated with new location, where new data were not written yet. For this MapBD storage is protected by write-ahead-log (WAL) which replies commits in atomic fashion. WAL is reliable and simple, and is used by many databases such as Posgresql or Mysql.</p>
<p>However WAL is slow, data has to be copied and synced multiple times. You may optionally disable WAL by disabling transactions: <code>DBMaker.transactionDisable()</code>. In this case you <strong>must</strong> correctly close store before JVM shutdown, or you loose all your data. You may also use shutdown hook to close database before JVM exits, however this does not protect your data if JVM crashes or is killed:</p>
<pre><code>  DB db = DBMaker
    .transactionDisable()
    .closeOnJvmShutdown()
    .make()</code></pre>
<p>Transaction disable (also called direct mode) will apply all changes directly into storage file. Combine this with in-memory store or mmap files and you get very fast storage. Typical use is in scenario where data does not have to be persisted between JVM restarts or can be easily recreated: off-heap caches.</p>
<p>Other important usage is for initial data import. Transactions and no-transactions share the same storage format (except WAL) so one can import data very fast with transactions disabled. Once import is finished, store gets reopened with transactions enabled.</p>
<p>With transactions disabled you loose rollback capability, <code>db.rollback()</code> will throw an exception. <code>db.commit()</code> will have nothing to commit (all data are already stored), so it does next best thing: Commit tries to flush all write caches and synchronizes storage files. So if you call <code>db.commit()</code> and do not make any more writes, your store should be safe (no data loss) in case of JVM crash.</p>
<h2 id="Memory_mapped_files_mmap"><a href="#Memory_mapped_files_mmap">Memory mapped files (mmap)</a></h2>
<p>MapDB was designed from ground to take advantage of mmap files. However mmap files are limited to 2GB by addressing limit 32bit JVM. Mmap have lot of nasty effects on 32bit JVM, so by default we use slower and safer disk access mode called Random-Access-File (RAF).</p>
<p>Mmap files are much faster compared to RAF. Exact speed bonus depends on operating system and disk case management, but is typically between 10% and 300%.</p>
<p>Memory mapped files are activated with <code>DBMaker.mmapFileEnable()</code> setting.</p>
<p>One can also activate mmap files only if 64bit platform is detected: <code>DBMaker.mmapFileEnableIfSupported()</code>.</p>
<p>And finally you can take advantage of mmap files in 32bit platforms by using mmap file only for small but frequently used part of storage: <code>DBMaker.mmapFileEnableIfSupported()</code></p>
<p>Mmap files are highly dependent on operating system. For example on Windows you can not delete mmap file while it is locked by JVM. If Windows JVM dies without closing mmap file, you have to restart Windows to release file lock.</p>
<h2 id="CRC32_checksums"><a href="#CRC32_checksums">CRC32 checksums</a></h2>
<p>You may want to protect your data from disk corruption. MapDB optionally supports CRC32 checksums. In this case each record stores extra 4 bytes which contains its CRC32 checksum. If data are somehow modified or corrupted (file system or storage error), next read will fail with an exception. This gives early warning and prevents db from returning wrong data.</p>
<p>CRC32 checksum has to be calculated on each put/modify/get operation so this option has some performance overhead. Write-ahead-log uses CRC32 checksum by default.</p>
<p>Checksum is activated by this setting: <code>DBMaker.checksumEnable()</code>. It affect storage format, so once activate you always have to reopen store with this setting. Also checksum can not be latter activate once store was created without it.</p>
<p>TODO: CRC32 serializer link</p>
<p>TODO: WAL and CRC32 disable</p>
<p>TODO: WAL flush on commit</p>
<p>TODO async file sync, use futures?</p>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
<li><a href="#caches" title="Caches">Caches</a></li>
<li><a href="#Hash_Table_cache" title="Hash Table cache">Hash Table cache</a></li>
<li><a href="#Least-Recently-Used_cache" title="Least-Recently-Used cache">Least-Recently-Used cache</a></li>
<li><a href="#Hard_reference_cache" title="Hard reference cache">Hard reference cache</a></li>
<li><a href="#Soft_and_Weak_reference_cache" title="Soft and Weak reference cache">Soft and Weak reference cache</a></li>
<li><a href="#Disabled_cache_or_reduce_size" title="Disabled cache or reduce size">Disabled cache or reduce size</a></li>
<li><a href="#Clear_cache" title="Clear cache">Clear cache</a></li>
<li><a href="#Cache_hit_and_miss_statistics" title="Cache hit and miss statistics">Cache hit and miss statistics</a></li>
<li><a href="#Cache_priority" title="Cache priority">Cache priority</a></li>
</ul>
<h1 id="caches"><a href="#caches">Caches</a></h1>
<p>MapDB has several options to cache deserialized objects. Proper cache configuration is crucial for good performance of your application. Many performance problems can be fixed just by changing cache settings.</p>
<p>Most dbs and old generation of MapDB (JDBM) use fixed size to cache read from disk. MapDB eliminated page layer, so it does not have regular cache. Instead it used memory mapped files and relies on operating system to do disk caching.</p>
<p>When we talk about cache in mapdb we mean <em>instance cache</em> .Instead of pages, MapDB caches deserialized objects such as tree nodes, <code>Long</code>, <code>Person</code> and so on. Instance cache helps to minimize deserialization overhead. When you fetch an object twice, it will be deserialized only once, second time it will be fetched from cache.</p>
<p>Because object instance may be stored in cache, yor data-model has to be immutable. On every modification you must create copy of object and persist new version. You also can not modify object fetched from db, an example:</p>
<pre><code>    //wrong
    Person person = new Person();
    map.put(&quot;John&quot;,person);
    person.setName(&quot;John&quot;);

    //right
    Person person = new Person();
    person.setName(&quot;John&quot;);
    map.put(&quot;John&quot;,person);

    //wrong
    Person person = map.get(&quot;John&quot;);
    person.setAge(15);

    //right
    Person person = map.get(&quot;John&quot;);
    person = person.clone();
    person.setAge(15);
    map.put(&quot;John&quot;,person);
</code></pre>
<p>MapDB offers 5 cache implementations. Some are unbounded and could cause <code>OutOfMemoryError</code> when used incorrectly.</p>
<h2 id="Hash_Table_cache"><a href="#Hash_Table_cache">Hash Table cache</a></h2>
<p>This cache is fixed size hash table (an array), where elements are placed by recid hash. Old entries are evicted by hash collisions. It has almost zero performance overhead, but provides good results, so this cache is on by default.</p>
<p>It does not have automatic entry eviction, so some records may remain in cache (on heap) for very long time. This cache should be used in combination with small records, there is no auto-removal and it could cause <code>OOME</code></p>
<p>Default cache size is 32000 records, there is DBMaker parameter to regulate its size. This cache is on by default, so it does not need to be enabled in DBMaker. An example:</p>
<pre><code>    DB db = DBMaker
        .newFileDB(file)          //or memory db
        .cacheSize(1000000)     //optionally change cache size
        .make()
</code></pre>
<h2 id="Least-Recently-Used_cache"><a href="#Least-Recently-Used_cache">Least-Recently-Used cache</a></h2>
<p>LRU cache keeps track when records are used, and if cache would grow beyond maximal size, it removes least recently used records. Compared to HashTable cache it has better hit statistics, but more overhead. There is overhead associated with maintaining LRU queue. It is recommended to use this cache only if cost of deserialization cost is high and cache miss is serious problem.</p>
<p>MRU queue is maintained on ‘best effort’, there are some shortcuts for better performance. So the actual cache size oscillates a few records around maximal size. Please consult <code>LongConcurrentLRUMap</code> source for example.</p>
<p>This cache is activated by <code>cacheLRUEnable()</code> DBMaker parameter. You can also change maximal size, default size is 32000 records. An example:</p>
<pre><code>    DB db = DBMaker
        .newFileDB(file)        //or memory db
        .cacheLRUEnable()
        .cacheSize(1000000)     //optionally change cache size
        .make()
</code></pre>
<h2 id="Hard_reference_cache"><a href="#Hard_reference_cache">Hard reference cache</a></h2>
<p>HardRef cache is unbounded cache which does not evict any of its entries. This cahce is practically <code>Map</code> of recids and records: <code>HashMap&lt;recid, Record&gt;</code></p>
<p>If store is larger than heap, it will get filled and eventually cause <code>OOME</code> exception. It is great to get great performance from small stores. After cache is warmed, it offers read-only performance comparable to <code>java.util</code> collections.</p>
<p>MapDB also has weak/soft reference caches, but GC has some serious overhead. So hard reference cache has lower overhead if there is enough memory.</p>
<p>This cache is activated with <code>cacheHardRefEnable()</code> DBMaker parameter. It does not have maximal size. An example:</p>
<pre><code>    DB db = DBMaker
        .newFileDB(file)          //or memory db
        .cacheHardRefEnable()
        .make()
</code></pre>
<p>No records are automatically removed from this cache. But you can still clear all records manually:</p>
<pre><code>    db.getEngine().clearCache();</code></pre>
<h2 id="Soft_and_Weak_reference_cache"><a href="#Soft_and_Weak_reference_cache">Soft and Weak reference cache</a></h2>
<p>Other option is to use weak or soft reference cache. In this cache garbage collector removes records from cache. This cache is practically <code>Map</code> of recids and references to records: <code>HashMap&lt;recid, SoftRef&lt;Record&gt;&gt;</code>.</p>
<p>Soft and Weak references differs by eagerness with which they are garbage collected. Weak reference should be GC immediately after all references are released. Soft reference should be only removed when free heap is low. However practical implications depends on JVM settings. For example you can still get <code>OutOfMemoryError</code> even with soft references.</p>
<p>This caches are activated by <code>cacheWeakRefEnable()</code> and <code>cacheSoftRefEnable()</code> DBMaker parameters:</p>
<pre><code>    //weak
    DB db = DBMaker
        .newFileDB(file)            //or memory db
        .cacheWeakRefEnable()
        .make()

    //or soft
    DB db = DBMaker
        .newMemoryDB()              //or file db
        .cacheSoftRefEnable()
        .make()
</code></pre>
<h2 id="Disabled_cache_or_reduce_size"><a href="#Disabled_cache_or_reduce_size">Disabled cache or reduce size</a></h2>
<p>Instance cache is enabled by default. On small devices you may want to disable it to reduce memory usage. This is done by <code>cacheDisable()</code> parameter:</p>
<pre><code>    DB db = DBMaker
        .newFileDB(file)          //or memory db
        .cacheDisable()
        .make()
</code></pre>
<p>Completely disabling cache hurts performance. So there are could be better alternatives:</p>
<p>First you could clear cache after every operation. Checkout howto clearc cache in chapter bellow:</p>
<p>Other alternative is to reduce cache size. By default cache size is 32,000 records, probably too much for most Android phones:</p>
<pre><code>    DB db = DBMaker
        .newFileDB(file)        //or memory db
        .cacheSize(128)         //optionally change cache size
        .make()</code></pre>
<h2 id="Clear_cache"><a href="#Clear_cache">Clear cache</a></h2>
<p>If you only use MapDB in batches, you can reduce cache memory overhead, by clearing cache at end of batch:</p>
<pre><code>    //do some heavy stuff with mapdb:
    map.getAll...

    //we are done clear cache
    db.getEngine().clearCache();

    //now do some other stuff, which does not use MapDB:
    save_my_files...
</code></pre>
<h2 id="Cache_hit_and_miss_statistics"><a href="#Cache_hit_and_miss_statistics">Cache hit and miss statistics</a></h2>
<p>Right now MapDB does not offer hit/miss statistics for any cache. This feature requires a few lines of code and will be added soon.</p>
<p>TODO cache hit/miss statistics.</p>
<h2 id="Cache_priority"><a href="#Cache_priority">Cache priority</a></h2>
<p>TODO Right now there is no record priority for cache.</p>
<p>However it is very easy to add this into MapDB. For example you could give more preferential treatment to btree directory nodes, while leaf nodes would not be cached. All it takes is a few <code>record instanceof BTreeDirNode</code> in single class.</p>
<p>TODO MapDB could also have flexible multi level cache layering.</p>
<hr />
<ul>
</ul>
<p>//TODO better name, perhaps ‘user story’?</p>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
<li><a href="#btreemap" title="BTreeMap">BTreeMap</a></li>
</ul>
<h1 id="btreemap"><a href="#btreemap">BTreeMap</a></h1>
<hr />
<ul>
<li><a href="#htreemap" title="HTreeMap">HTreeMap</a></li>
<li><a href="#Parameters" title="Parameters">Parameters</a></li>
<li><a href="#Entry_expiration_parameters" title="Entry expiration parameters">Entry expiration parameters</a></li>
<li><a href="#Compared_to_BTreeMap" title="Compared to BTreeMap">Compared to BTreeMap</a></li>
</ul>
<h1 id="htreemap"><a href="#htreemap">HTreeMap</a></h1>
<p>HTreeMap (aka HashMap) is one of <code>Map</code>s offered by MapDB. It has great performance with large keys. It also offers entry expiration if time-to-live or maximal size is exceeded.</p>
<p>HTreeMap is <em>segmented Hash Tree</em>. Most hash collections use an array for hash table, which requires copying all data when hash table is resized. HTreeMap uses auto-expanding 4 level tree, so it never needs resizing.</p>
<p>To support concurrency HTreeMap is split to 16 independent segments, each with separate read-write lock. <code>ConcurrentHashMap</code> works similar way. Number of segments (also called concurrency factor) is hard wired into design and can not be changed.</p>
<p>HTreeMap optionally supports entry expiration based on four criteria: maximal map size, time-to-live since last modification and time-to-live since last access. Expired entries are automatically removed. This feature uses FIFO queue, each segment has independent expiration queue. Priority per entry can not be set.</p>
<h2 id="Parameters"><a href="#Parameters">Parameters</a></h2>
<p>HTreeMap has number of parameters to tune its performance. Number of segments (aka concurrency factor) is hard-coded to16 and can not be changed. Other params can be set only when map is created and can not be changed latter.</p>
<p>Most important are probably <strong>serializers</strong>. General serialization has some guessing and overhead, so it always has better performance to use more specific serializers. To specify key and value serializer use code bellow. There are dozens ready to use serializers available as static fields on <code>Serializer</code> interface:</p>
<pre><code>    HTreeMap&lt;String, Long&gt; map = db.createHashMap(&quot;map&quot;)
        .keySerializer(Serializer.STRING)
        .valueSerializer(Serializer.LONG)
        .makeOrGet()</code></pre>
<p>HTreeMap is recommended for handling large key/values. In same cases you may want to use compression. Enabling compression store-wide is not always best, since constantly (de)compressing index tree has overhead. Instead it is better to apply compression just to specific large key. This is done by using serializer wrapper:</p>
<pre><code>    HTreeMap&lt;String, Long&gt; map = db.createHashMap(&quot;map&quot;)
        .keySerializer(new Serializer.CompressionWrapper(Serializer.STRING))
        .makeOrGet()</code></pre>
<p>Other useful parameter is <strong>Hasher</strong>. By default HTreeMap uses 32bit hash generated by <code>hashCode()</code>. Some classes just return memory pointer, which changes after serialization/deserialization, rendering it useless for persistence. Also it is not always necessary to calculate full hash of large objects (Strings), sometimes it is better to use weaker hash. So HTreeMap allows you to supply custom Hasher which will generate hash code for keys and decide which keys are equal. This way you can use primitive arrays as key without wrapper:</p>
<pre><code>    HTreeMap&lt;byte[], Long&gt; map = db.createHashMap(&quot;map&quot;)
        .keySerializer(Serializer.BYTE_ARRAY)
        .hasher(Hasher.BYTE_ARRAY)
        .makeOrGet()</code></pre>
<p>Other parameter is <strong>size counter</strong>. By default HTreeMap does not keep track of its size, calling <code>map.size()</code> requires linear scan to count all entries. You can change it by enabling size counter, in that case <code>map.size()</code> is instant, but there is some overhead on inserts.</p>
<pre><code>    HTreeMap&lt;String, Long&gt; map = db.createHashMap(&quot;map&quot;)
        .counterEnable()
        .makeOrGet()</code></pre>
<p>And finally some sugar. There is <strong>value creator</strong>, a function to create value if existing value is not found. Newly created value is inserted into map. This way <code>map.get(key)</code> never returns null. This is mainly useful for various generators and caches.</p>
<pre><code>    HTreeMap&lt;String,Long&gt; map = db.createHashMap(&quot;map&quot;)
            .valueCreator(new Fun.Function1&lt;Long,String&gt;() {
                @Override public Long run(String o) {
                    return 1111L;
                }
            })
            .makeOrGet();</code></pre>
<p>or more readable version in Java 8:</p>
<pre><code>    HTreeMap&lt;String,Long&gt; map = db.createHashMap(&quot;map&quot;)
            .valueCreator((key)-&gt; 1111L)
            .makeOrGet();

    // this way map.get() returns 1111L if no value is found
    map.get(&quot;aa&quot;); // 1111L
    map.get(&quot;bb&quot;); // 1111L

    // map now contains [&quot;aa&quot;-&gt;1111L, &quot;bb&quot;-&gt;1111L]</code></pre>
<h2 id="Entry_expiration_parameters"><a href="#Entry_expiration_parameters">Entry expiration parameters</a></h2>
<p><code>HTreeMap</code> offers optional entry expiration if some conditions are met. Entry can expire if:</p>
<ul>
<li><p>Number of entries in map would exceed maximal size</p></li>
<li><p>Entry exist in map longer time than expiration period is. The expiration period could be since last modification or last read access.</p></li>
<li><p>Disk/memory space consumed by Map is bigger then some limit in GB.</p></li>
</ul>
<p>There is shortcut in <code>DBMaker</code> to quickly use <code>HTreeMap</code> as off-heap cache with memory size limit:</p>
<pre><code>    // Off-heap map with max size 16GB
    Map cache = DBMaker
        .newCacheDirect(16)</code></pre>
<p>This equals to <code>expireStoreSize</code> param:</p>
<pre><code>    HTreeMap cache = db.createHashMap(&quot;cache&quot;)
        .expireStoreSize(128)
        .makeOrGet()</code></pre>
<p>It is also possible to limit maximal size of map:</p>
<pre><code>    HTreeMap cache = db.createHashMap(&quot;cache&quot;)
        .expireMaxSize(128)
        .makeOrGet()</code></pre>
<p>And finally you can set expiration time since last modification or since last access.</p>
<pre><code>    // remove all entries 1H after last modification, or 10 minutes after last get()
    HTreeMap cache = db.createHashMap(&quot;cache&quot;)
         .expireAfterWrite(1, TimeUnit.HOURS)
         .expireAfterRead(10, TimeUnit.MINUTES)
         .makeOrGet()</code></pre>
<h2 id="Compared_to_BTreeMap"><a href="#Compared_to_BTreeMap">Compared to BTreeMap</a></h2>
<p>HTreeMap has one major advantage for using with large keys. Unlike BTreeMap it only stores hash codes in tree nodes. Each lookup on BTreeMap deserializes number of tree nodes together with their keys.</p>
<p>TODO link to performance test, compare with BTreeMap</p>
<p>On other side HTreeMap has limited concurrency factor to 16, so its writes wont scale over 4 CPU cores. It uses read-write locks, so read operations are not affected. However in practice disk IO is more likely to be bottleneck.</p>
<hr />
<ul>
<li><a href="#queues" title="Queues">Queues</a></li>
<li><a href="#FIFO_Queue" title="FIFO Queue">FIFO Queue</a></li>
<li><a href="#Stack" title="Stack">Stack</a></li>
<li><a href="#CircularQueue" title="CircularQueue">CircularQueue</a></li>
</ul>
<h1 id="queues"><a href="#queues">Queues</a></h1>
<p>MapDB has three <code>Queue</code> implementations:</p>
<ul>
<li>Queue aka First-In-First-Out</li>
<li>Stack aka Last-In-First-Out</li>
<li>CircularQueue with limited size</li>
</ul>
<p>Lock-free queues in MapDB have several limitations, for example it is not possible to count elements without actually removing them. Queues in MapDB usually implement only minimal subset from <code>BlockingQueue</code> interface. In future we will introduce <code>List</code> which will fully implement <code>BlockingDequeue</code> and <code>List</code> interfaces, this will use global <code>ReadWriteLock</code>.</p>
<p>To instantiate queue use get method:</p>
<pre><code>    // first-in-first-out queue
    BlockingQueue fifo = db.getQueue(“fifo”);

    // last-in-first-out queue (stack)
    BlockingQueue lifo = db.getStack(“lifo”);

    // circular queue with limited size
    BlockingQueue c = db.getCircularQueue(“circular”);</code></pre>
<h2 id="FIFO_Queue"><a href="#FIFO_Queue">FIFO Queue</a></h2>
<p>This only takes two extra parameters. Fist you can supply custom serializers used on entries.</p>
<p>Second param decides lock based eviction. Lock-free (false) means better concurrency, but places tomb-stones in place of removed entries, over time the store size grows and will need compaction. With locks (true) the removed entries are deleted under global locks, so there is concurrency penalty.</p>
<pre><code>    // first-in-first-out queue
    BlockingQueue&lt;String&gt; fifo = db.createQueue(“fifo”, Serializer.STRING, false);</code></pre>
<h2 id="Stack"><a href="#Stack">Stack</a></h2>
<p>This only takes two extra parameters. Fist you can supply custom serializers used on entries.</p>
<p>Second param decides lock based eviction. Lock-free (false) means better concurrency, but places tomb-stones in place of removed entries, over time the store size grows and will need compaction. With locks (true) the removed entries are deleted under global locks, so there is concurrency penalty.</p>
<pre><code>    // last-in-first-out queue
    BlockingQueue&lt;String&gt; stack = db.createStack(“stack”, Serializer.STRING, false);</code></pre>
<h2 id="CircularQueue"><a href="#CircularQueue">CircularQueue</a></h2>
<p>This only takes two extra parameters. Fist you can supply custom serializers used on entries.</p>
<p>Second parameter is queue size. If number of entries exceeds the queue size, some entries will get overwritten and lost.</p>
<pre><code>    // circular queue
    BlockingQueue&lt;String&gt; circular = db.createCircularQueue(“circular”, Serializer.STRING, 10000);</code></pre>
<hr />
<ul>
</ul>
<hr />
<ul>
<li><a href="#secondary_collections" title="Secondary Collections">Secondary Collections</a></li>
<li><a href="#Consistency" title="Consistency">Consistency</a></li>
<li><a href="#Performance" title="Performance">Performance</a></li>
</ul>
<h1 id="secondary_collections"><a href="#secondary_collections">Secondary Collections</a></h1>
<p>Rational databases have very good system of primary and secondary indexes, tables and views. It has clear benefits for extensibility, clarity and robustness. On other side it has limitations for scalability and performance. MapDBs Secondary Collections are <em>poor man`s</em> SQL tables. It brings most benefits without sacrificing flexibility.</p>
<p>Secondary Collections are very simple and flexible way to access and expand primary collections. Primary collection is authoritative source of data, and is modified by user. Secondary collection contains records derived from primary. Secondary is bind to primary and updated by listener when primary collection is modified. Secondary should not be modified directly by user.</p>
<p>Secondary Collections are typically used in three ways:</p>
<ul>
<li><p>Secondary Keys (indexes) to efficiently access records in primary collection.</p></li>
<li><p>Secondary Values to expand primary collection, while keeping primary small and fast.</p></li>
<li><p>Aggregation to groups. For example to list all high-income customers.</p></li>
</ul>
<p>Primary Collection is typically stored in MapDB. The requirement is that it provides <a href="/apidocs/org/mapdb/Bind.MapWithModificationListener.html">modification listener</a> triggered on entry update, insert or removal. There is no such requirement for Secondary Collection. Secondary may be stored in MapDB, but it can also be usual Java Collection such as java.util.TreeSet.</p>
<p>Primary and Secondary collection are bind together. There is <a href="http://www.mapdb.org/apidocs/org/mapdb/Bind.html">Bind</a> class with static methods to establish binding. Binding adds modification listener to primary collection and changes secondary collection accordingly. It also removes old entries from secondary if primary entry gets deleted or modified.</p>
<p>Bind relation is not persistent, so binding needs to be restored every time store is reopened. If secondary collections is empty when binded, entire primary is traversed and secondary is filled accordingly.</p>
<h2 id="Consistency"><a href="#Consistency">Consistency</a></h2>
<p>Consistency between primary and secondary collections is on ‘best-effort’ basis. Two concurrent threads might observe secondary contains old values while primary was already updated. Also if secondary is on heap, while primary is in transactional store which gets rolled back, secondary will become inconsistent with primary (its changes were not rolled back.</p>
<p>Secondary collection is updated in <a href="https://en.wikipedia.org/wiki/Serializability">serializable fashion</a>. This means that if two concurrent thread update primary, secondary collection is updated with ‘winner’. TODO verify this is true, update this paragraph after <a href="https://github.com/jankotek/MapDB/issues/226">issue</a> is closed.</p>
<p>There are some best practices for Secondary Collections to handle this:</p>
<ul>
<li><p>Secondary Collections must be thread safe. Either use MapDB or <code>java.util.concurrent.*</code> collections. Other (but not optimal) option is to use <code>Collections.synchronized*()</code> wrappers.</p></li>
<li><p>When using concurrent transactions, do not mix collections from multiple transactions. If primary gets rollback, secondary will not be updated if its not within the same transaction.</p></li>
<li><p>Keep binding minimal. It should only transform one value into other, without dependency on third collections.</p></li>
</ul>
<h2 id="Performance"><a href="#Performance">Performance</a></h2>
<p>To import large dataset, you should not enable binding until primary collection has finished its import. Also there might be more efficient way to pre-fill secondary collection (for example with data pump).</p>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
<hr />
<ul>
</ul>
</body>
</html>
