HTreeMap
========

HTreeMap provides ``HashMap`` and ``HashSet``. 
It has a great performance with large keys.
It also offers entry expiration based on the size or time-to-live

HTreeMap is a *segmented Hash Tree*. Most hash collections use hash table based in fixed size array and
when it becomes full, all data has to be moved and rehashed into 
a new, bigger table. HTreeMap uses auto-expandind Hash Treestructure, so it never needs resizing.
It also occupies less space, since empty hash slots do not consume any space.
On the other hand, the tree structure requires more seeks and is slower on access.
Its performance degrades with size, but the maximal dir node size is 128,
so degradation is very small. 
TODO performance degradation depending on size. Probably  log N.

To achieve parallel scalability, HTreeMap is split into 16  segments,
each with a separate read-write lock. ``ConcurrentHashMap`` in JDK 7 works in a similar
way. The number of segments (also called concurrency scale) is hard wired
into design and cannot be changed.
Because all segments share an underlying storage, concurrent scalability is not perfect.
Another option is to create each segment with separate storage.

HTreeMap optionally supports entry expiration based on four criteria:
maximal map size, time-to-live since last modification and time-to-live
since last access. Expired entries are automatically removed. This
feature uses FIFO queue and each segment has independent expiration queue.
Priority per entry cannot be set.

Parameters
----------

HTreeMap has a number of parameters to tune its performance. The number of
segments (aka concurrency factor) is hard-coded to 16 and cannot be
changed. Other params can be set only when the map is created and cannot be
changed latter.

The most important are **serializers**. General serialization has
some guessing and overhead, so it always has better performance if
more specific serializers are used. To specify a key and value serializer, use the code
bellow. There are dozens ready to use serializers available as static
fields on ``Serializer`` interface:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_serializer.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

HTreeMap is recommended for handling large key/values. In same cases you
may want to use compression. Enabling compression store-wide is not
always the best option, since a constantly (de)compressing index tree has overhead.
Instead, it is better to apply compression just to a specific serializer on key or value.
This is done by using serializer wrapper:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_compressed.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8
            
Most hash maps uses 32bit hash generated by ``Object.hashCode()`` and check equality with ``Object.equals(other)``. 
However many classes do not implement those functions correctly, and inconsistent hashing is 
very bad for persistence, as it could cause data loss.
By default, configuration HTreeMap uses a generic key serializer and relies on those methods as well.
However, it throws an ``IllegalArgumentException`` if inconsistent hashing is detected (for example ``byte[]``
is used without serializer).

If the specialized Key Serializer is defined, HTreeMap relies on it to provide hash code and equality check
for keys. For example ``Serializer.BYTE_ARRAY`` uses  ``java.util.Arrays.hashCode(byte[])`` to generate hash code.
This way you can use primitive arrays directly as a key/value without a wrapper. 
Bypassing wrappers such as ``String``, improves performance:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_byte_array.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8


Another parameter is the **size counter**. By default HTreeMap does not keep
track of its size and calling ``map.size()`` requires a linear scan to count
all entries. You can enable size counter and in that case
``map.size()`` is instant, but there is some overhead on inserts.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_counter.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

And finally some sugar. There is **value creator**, a function to create
a value if the existing value is not found. A newly created value is inserted
into the map. This way ``map.get(key)`` never returns null. This is mainly
useful for various generators and caches.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_value_creator.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8


Or a more readable version in Java 8:

.. code:: java

        HTreeMap<String,Long> map = db.hashMapCreate("map")
                .valueCreator((key)-> 1111L)
                .makeOrGet();

        // this way map.get() returns 1111L if no value is found
        map.get("aa"); // 1111L
        map.get("bb"); // 1111L

        // map now contains ["aa"->1111L, "bb"->1111L]

Entry expiration parameters
---------------------------

``HTreeMap`` offers optional entry expiration if some conditions are
met. Entry can expire if:

-  The number of entries in a map would exceed the maximal size

-  An entry exists in the map longer time than the expiration period is. The
   expiration period could be since the last modification or since the last read
   access.

-  Disk/memory space consumed by Map is bigger then some limit in GB.

There is a shortcut in ``DBMaker`` to quickly use ``HTreeMap`` as an off-heap
cache with memory size limit:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_space_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

This equals to ``expireStoreSize`` param:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_space_limit2.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

It is also possible to limit the maximal size of a map:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_size_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

And finally you can set an expiration time since the last modification or since
the last access.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_ttl_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

TODO expiration counts are approximate. Map size can go slightly over limits for short period of time.

TODO disk space limit has issues. Investigate how it works and document

TODO expiration threads single and multithreaded.

Binding and overflow
-----------------------

Maps in MapDB support call back notification on insert, update and removal.
There is ``Bind`` class (TODO link to chapter), which links two collections together and keeps them synchronized.
There are several ways to associate collections but this is described in a separate chapter (TODO link).

Specific for ``HTreeMap`` is the expiration overflow. It is possible tokeep the most recently used
entries in memory for faster access. After the entry expires, it is moved to a slower storage on disk.

To create overflow you need two maps. One on-disk and one in-memory. You bind them together with the
``.expireOverflow(onDisk, true)`` parameter in builder:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_init.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Once binding is established, every entry removed from ``inMemory`` map will be added to ``onDisk`` map.
This applies to expired entries, but also entries removed using the ``map.remove()`` method.
To completely remove entry from both maps, one must first remove from ``inMemory`` and then from ``onDisk``:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_remove.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

If the ``inMemory.get(key)`` is called, one might expect null, and than check ``onDisk``. However to make things simpler
``inMemory`` has a value creator. So ``inMemory.get(key)`` returns values from both ``inMemory`` and ``onDisk``.
If value is not found, it checks the second map and adds its value into ``inMemory``.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_get.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Get and removal synchronizes two collections in a cyclic way. The entry gets moved into ``onDisk`` when its deleted from ``inMemory``.
And ``inMemory`` gets populated from ``onDisk`` when get method does not find a value in-memory. This creates a consistency problem,
as to which map is authoritative? Which contains the 'real' data?

MapDB offers three alternatives. The method ``.expireOverflow(onDisk, true)`` takes a boolean parameter to control
how expiration behaves. With ``false`` expiration it uses ``map.put(key,value)`` to insert data to inDisk.
With ``true`` it uses ``map.putIfAbsent(key,value``, so no existing value gets overwritten.

The first option is ``true``. It means that ``inMemory`` is authoritative and ``onDisk`` content will get
overwritten once the data expires from memory and overflows to disk:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_main_inmemory.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

With ``false``, the data added into ``inMemory`` are not considered authoritative. OnDisk content will never get
overwritten. This also adds a performance bonus on expiration if the values are the same, since on-disk values
are not overwritten by equal values.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_main_ondisk.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

There is a question into which collection new data should be inserted. It depends how much you value your data.
If it caches some external source (such as SQL), I would insert data into ``inMemory`` and let them hoover in air.
In this case data might be inconsistent after crash/shutdown. It is better to drop the cache content and
repopulate it from the primary store.

If thee content of the cache is valuable (is primary content, or too high cost to repopulate from SQL)
one should insert data to ``onDisk``.
Disk store can be protected by transaction with periodic commit.
In this case only a small time interval of data would be lost.

Inserts are simple, but updates create a consistency problem. If the key-value pair changes, one collection might contain
an older value. So on updates it is recommended to update BOTH ``inMemory`` and ``onDisk`` collections:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_update.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

The third option is to use listeners and ``Bind`` utilities yourself.

Concurrent scalability
------------------------

HTreeMap scales concurrently by using 16 separate segments, each with its own ``ReadWriteLock``.
Each segment has its own independent state, hash tree and also expiration queue.
But all segments still share underlying storage and are limited by its performance. 

There is an option to shard HTreeMap. Each separate segment can get its own storage, so no shared
state exist between segments. This way one can get linear concurrent scalability which corresponds
to 16 segments. TODO benchmarks.

Trade off in this case is a higher memory consumption. There are 16 different stores, each with its own
memory allocator and unused blocks. TODO memory benchmarks. 
But each store can be compacted separately. TODO add compaction doc for this 

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_segmented.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Compared to BTreeMap
--------------------

`HTreeMap` has a major advantage over `BTreeMap` with large keys. Unlike
BTreeMap, it only stores hash codes in tree nodes.
BTreeMap deserializes tree nodes, together with their keys, on each lookup.
A simple ``BTreeMap.get(key)`` could deserialize houndreds of keys.

TODO link to performance test, compare with BTreeMap

On the other side HTreeMap has a limited concurrency factor to 16, so with
writes it wont scale over 4 CPU cores. It uses read-write locks, so read
operations are not affected. However, in practice the disk IO is more likely
to be bottleneck. TODO benchmarks

HTreeMap can be easily sharded by segments. For in-memory map it might have better 
concurrent scalability.

HTreeMap is simpler than BTreeMap. It has more predictable performance over a long period of time
and it does not get fragmented after frequent deletes.
HTreeMap also offers expiration. 
BTreeMap pays tax in some cases for its complex lock-free design.
